version: '3.8'

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - data-pipeline-net

  redis:
    image: redis:6
    networks:
      - data-pipeline-net

  airflow-webserver:
    image: apache/airflow:2.9.1
    restart: always
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__BROKER_URL: redis://redis:6379/0
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8082:8080"
    command: webserver
    networks:
      - data-pipeline-net

  airflow-scheduler:
    image: apache/airflow:2.9.1
    restart: always
    depends_on:
      - airflow-webserver
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__BROKER_URL: redis://redis:6379/0
    volumes:
      - ./dags:/opt/airflow/dags
    command: scheduler
    networks:
      - data-pipeline-net

  airflow-flower:
    image: apache/airflow:2.9.1
    depends_on:
      - airflow-webserver
      - airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    ports:
      - "5555:5555"
    command: flower
    networks:
      - data-pipeline-net

  hadoop:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop
    ports:
      - "9870:9870"
      - "8088:8088"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop:9000  # 수정된 부분
      - CORE_CONF_hadoop_security_authentication=simple
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    volumes:
      - ./data/hadoop:/hadoop/dfs
    networks:
      - data-pipeline-net

  zookeeper:
    container_name: zookeeper-container
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
    networks:
      - data-pipeline-net

  kafka:
    container_name: kafka-container
    image: wurstmeister/kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.1.133:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-container:2181
      KAFKA_CREATE_TOPICS: "test-topic:1:1,marketing.analysis.request:1:1"
    networks:
      - data-pipeline-net

  spark-master:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./:/opt/bitnami/spark/jobs
    networks:
      - data-pipeline-net

  spark-worker-1:
    image: bitnami/spark:latest
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    networks:
      - data-pipeline-net

networks:
  data-pipeline-net:
    driver: bridge

volumes:
  postgres-db-volume:
    driver: local